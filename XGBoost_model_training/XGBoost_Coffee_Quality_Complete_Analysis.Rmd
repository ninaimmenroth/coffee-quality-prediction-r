---
title: "XGBoost Coffee Quality Prediction"
subtitle: "Comprehensive Analysis with Multiple Modeling Approaches"
abstract: |
  This report analyzes the Coffee Quality Institute's Arabica coffee dataset using 
  XGBoost regression to predict Cupper Points (expert quality scores). The analysis 
  compares multiple feature sets (Sensory, Extended, Full), evaluates baseline models, 
  and documents both successful approaches and those found less effective (Lambda/Min_Child_Weight 
  regularization, Bayesian Optimization). Key finding: sensory features dominate prediction, 
  with XGBoost providing modest but statistically significant improvements over Linear Regression.
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document:
    toc: true
    number_sections: true
  word_document:
    toc: true
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
# CLEAR ENVIRONMENT
rm(list = ls(all.names = TRUE))

# LOAD PACKAGES
library(xgboost)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(knitr)
library(stringr)

# GLOBAL CHUNK OPTIONS
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.width = 8,
  fig.height = 5
)

# GGPLOT THEME
theme_set(theme_minimal())

# GLOBAL CONSTANTS
SEED <- 42
K_FOLDS <- 10
```

\newpage

# Introduction

This comprehensive report analyzes coffee quality prediction using XGBoost regression. The analysis incorporates multiple modeling approaches tested across several experiments.

**Successful Approaches:**

- Full model comparison (Sensory, Extended, Full feature sets vs baselines)
- Enhanced evaluation metrics (MAE, MAPE, Adjusted R², Max Error)
- Learning Curves for training dynamics visualization
- Exploratory Data Analysis with Correlation Analysis
- Predicted vs Actual diagnostic plots

**Approaches Tested but Found Less Effective:**

- Lambda (L2) and Min_Child_Weight regularization tuning
- Bayesian Optimization vs Grid Search comparison

## Configuration

```{r config, echo=FALSE}
config <- data.frame(
  Parameter = c("Random Seed", "Cross-Validation Folds", "Train/Test Split", 
                "Early Stopping Rounds", "Max Boosting Rounds"),
  Value = c(SEED, K_FOLDS, "80/20", "50", "2000")
)

kable(config, caption = "Analysis Configuration", format = "markdown")
```

\newpage

# Data Loading and Preprocessing

## Load Data

```{r load-data}
coffee_raw <- read.csv("arabica_data_cleaned.csv")
cat("Raw data:", nrow(coffee_raw), "rows,", ncol(coffee_raw), "columns\n")
```

## Clean Categorical Variables

```{r preprocess-categorical}
# Processing Method
coffee_raw$Processing.Method <- ifelse(
  is.na(coffee_raw$Processing.Method) | coffee_raw$Processing.Method == "", 
  "Unknown", coffee_raw$Processing.Method
)
coffee_raw$Processing.Method <- factor(coffee_raw$Processing.Method)

# Variety (Top 4 + Other)
variety_counts <- sort(table(coffee_raw$Variety), decreasing = TRUE)
top_4_varieties <- names(variety_counts)[1:4]
coffee_raw$Variety <- ifelse(
  is.na(coffee_raw$Variety) | coffee_raw$Variety == "" | 
    !(coffee_raw$Variety %in% top_4_varieties),
  "Other", coffee_raw$Variety
)
coffee_raw$Variety <- factor(coffee_raw$Variety)

# Country (Top 10 + Other)
country_counts <- sort(table(coffee_raw$Country.of.Origin), decreasing = TRUE)
top_10_countries <- names(country_counts)[1:10]
coffee_raw$Country.of.Origin <- ifelse(
  is.na(coffee_raw$Country.of.Origin) | 
    !(coffee_raw$Country.of.Origin %in% top_10_countries),
  "Other", coffee_raw$Country.of.Origin
)
coffee_raw$Country.of.Origin <- factor(coffee_raw$Country.of.Origin)

# Color
coffee_raw$Color <- ifelse(
  is.na(coffee_raw$Color) | coffee_raw$Color == "",
  "Unknown", coffee_raw$Color
)
coffee_raw$Color <- factor(coffee_raw$Color)

# In-Country Partner (Top 10 + Other)
partner_counts <- sort(table(coffee_raw$In.Country.Partner), decreasing = TRUE)
top_10_partners <- names(partner_counts)[1:10]
coffee_raw$In.Country.Partner <- ifelse(
  is.na(coffee_raw$In.Country.Partner) | 
    !(coffee_raw$In.Country.Partner %in% top_10_partners),
  "Other", coffee_raw$In.Country.Partner
)
coffee_raw$In.Country.Partner <- factor(coffee_raw$In.Country.Partner)

# Region (Top 15 + Other)
region_counts <- sort(table(coffee_raw$Region), decreasing = TRUE)
top_15_regions <- names(region_counts)[1:15]
coffee_raw$Region <- ifelse(
  is.na(coffee_raw$Region) | coffee_raw$Region == "" |
    !(coffee_raw$Region %in% top_15_regions),
  "Other", coffee_raw$Region
)
coffee_raw$Region <- factor(coffee_raw$Region)

cat("Categorical variables processed\n")
cat("  Processing.Method:", nlevels(coffee_raw$Processing.Method), "levels\n")
cat("  Variety:", nlevels(coffee_raw$Variety), "levels\n")
cat("  Country:", nlevels(coffee_raw$Country.of.Origin), "levels\n")
cat("  Color:", nlevels(coffee_raw$Color), "levels\n")
```

## Clean Numeric Variables

```{r preprocess-numeric}
# Altitude (Cap at 3000m and Impute)
coffee_raw$altitude_mean_meters <- ifelse(
  coffee_raw$altitude_mean_meters > 3000 & !is.na(coffee_raw$altitude_mean_meters),
  3000, coffee_raw$altitude_mean_meters
)

region_medians <- coffee_raw %>%
  filter(!is.na(altitude_mean_meters)) %>%
  group_by(Region) %>%
  summarise(region_alt = median(altitude_mean_meters), .groups = "drop")

global_median_alt <- median(coffee_raw$altitude_mean_meters, na.rm = TRUE)

coffee_raw <- coffee_raw %>%
  left_join(region_medians, by = "Region") %>%
  mutate(altitude_mean_meters = ifelse(
    is.na(altitude_mean_meters),
    ifelse(is.na(region_alt), global_median_alt, region_alt),
    altitude_mean_meters
  )) %>%
  select(-region_alt)

# Harvest Year Parsing
parse_harvest_year <- function(val) {
  if (is.na(val) || val == "") return(NA)
  match <- str_extract(as.character(val), "(20\\d{2}|19\\d{2})")
  if (!is.na(match)) return(as.integer(match))
  return(NA)
}

coffee_raw$Harvest.Year.Parsed <- sapply(coffee_raw$Harvest.Year, parse_harvest_year)
median_year <- median(coffee_raw$Harvest.Year.Parsed, na.rm = TRUE)
coffee_raw$Harvest.Year.Parsed <- ifelse(
  is.na(coffee_raw$Harvest.Year.Parsed), median_year, coffee_raw$Harvest.Year.Parsed
)

# Bag Weight Standardization
parse_bag_weight_kg <- function(val) {
  if (is.na(val)) return(NA)
  val <- tolower(as.character(val))
  num <- as.numeric(str_extract(val, "[\\d.]+"))
  if (is.na(num)) return(NA)
  if (grepl("lb", val)) return(num * 0.453592)
  if (grepl("kg", val)) return(num)
  return(ifelse(num > 10, num, NA))
}

coffee_raw$Bag.Weight.KG <- sapply(coffee_raw$Bag.Weight, parse_bag_weight_kg)
median_weight <- median(coffee_raw$Bag.Weight.KG, na.rm = TRUE)
coffee_raw$Bag.Weight.KG <- ifelse(
  is.na(coffee_raw$Bag.Weight.KG), median_weight, coffee_raw$Bag.Weight.KG
)

cat("Numeric variables processed\n")
cat("  Altitude missing:", sum(is.na(coffee_raw$altitude_mean_meters)), "\n")
cat("  Harvest.Year range:", min(coffee_raw$Harvest.Year.Parsed), "-", 
    max(coffee_raw$Harvest.Year.Parsed), "\n")
```

## Remove Leakage Columns

```{r remove-leakage}
coffee_raw$Total.Cup.Points <- NULL
coffee_raw$Altitude <- NULL
coffee_raw$Species <- NULL
coffee_raw$Harvest.Year <- NULL
coffee_raw$Bag.Weight <- NULL

cat("Leakage columns removed: Total.Cup.Points, Altitude, Species, Harvest.Year, Bag.Weight\n")
```

\newpage

# Exploratory Data Analysis

## Feature Descriptions

```{r feature-desc, echo=FALSE}
feature_desc <- data.frame(
  Feature = c("Aroma", "Flavor", "Aftertaste", "Acidity", "Body", "Balance",
              "Uniformity", "Clean.Cup", "Sweetness", "Moisture", 
              "Category.One.Defects", "Quakers"),
  Description = c("Smell/fragrance of the coffee",
                  "Overall taste profile",
                  "Lingering taste after swallowing",
                  "Bright, tangy quality",
                  "Weight/thickness on the tongue",
                  "How well flavors harmonize",
                  "Consistency across multiple cups",
                  "Absence of off-flavors/defects",
                  "Natural sweetness perception",
                  "Moisture content of beans (%)",
                  "Count of major defects",
                  "Unripe/underdeveloped beans count"),
  Scale = c(rep("0-10", 9), "%", "Count", "Count")
)

kable(feature_desc, caption = "Sensory Feature Descriptions", format = "markdown")
```

## Target Variable Analysis

```{r target-stats, echo=FALSE}
sensory_cols <- c("Aroma", "Flavor", "Aftertaste", "Acidity", "Body", 
                  "Balance", "Uniformity", "Clean.Cup", "Sweetness", 
                  "Moisture", "Category.One.Defects", "Quakers")

target_stats <- data.frame(
  Statistic = c("Mean", "Std Dev", "Median", "Min", "Max", "IQR"),
  Value = c(
    round(mean(coffee_raw$Cupper.Points, na.rm = TRUE), 4),
    round(sd(coffee_raw$Cupper.Points, na.rm = TRUE), 4),
    round(median(coffee_raw$Cupper.Points, na.rm = TRUE), 4),
    min(coffee_raw$Cupper.Points, na.rm = TRUE),
    max(coffee_raw$Cupper.Points, na.rm = TRUE),
    round(IQR(coffee_raw$Cupper.Points, na.rm = TRUE), 4)
  )
)
kable(target_stats, caption = "Target Variable (Cupper.Points) Summary", format = "markdown")
```

```{r target-histogram, echo=FALSE, fig.cap="Distribution of Cupper Points. Red dashed line indicates mean."}
ggplot(coffee_raw, aes(x = Cupper.Points)) +
  geom_histogram(bins = 30, fill = "#2E86AB", color = "white", alpha = 0.8) +
  geom_vline(xintercept = mean(coffee_raw$Cupper.Points, na.rm = TRUE), 
             color = "red", linetype = "dashed", linewidth = 1) +
  labs(x = "Cupper Points", y = "Count")
```

## Correlation Analysis

```{r correlation-table, echo=FALSE}
numeric_cols <- coffee_raw[, c(sensory_cols, "Cupper.Points")]
numeric_cols <- numeric_cols[complete.cases(numeric_cols), ]

correlations <- cor(numeric_cols)
target_corr <- correlations["Cupper.Points", ]
target_corr_sorted <- sort(target_corr[names(target_corr) != "Cupper.Points"], 
                           decreasing = TRUE)

corr_df <- data.frame(
  Feature = names(target_corr_sorted),
  Correlation = round(as.numeric(target_corr_sorted), 4)
)
kable(corr_df, caption = "Feature Correlations with Cupper.Points", format = "markdown")
```

```{r correlation-barplot, echo=FALSE, fig.cap="Feature correlations with Cupper Points."}
ggplot(corr_df, aes(x = reorder(Feature, Correlation), y = Correlation, 
                    fill = Correlation > 0)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "#2E86AB", "FALSE" = "#E63946"), 
                    guide = "none") +
  labs(x = "Feature", y = "Correlation") +
  geom_hline(yintercept = 0, linetype = "dashed")
```

```{r correlation-heatmap, echo=FALSE, fig.cap="Correlation heatmap of sensory features and target variable.", fig.width=10, fig.height=8}
cor_matrix <- cor(numeric_cols)
cor_melted <- as.data.frame(as.table(cor_matrix))
names(cor_melted) <- c("Var1", "Var2", "Correlation")

ggplot(cor_melted, aes(x = Var1, y = Var2, fill = Correlation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "#E63946", mid = "white", high = "#2E86AB",
                       midpoint = 0, limits = c(-1, 1)) +
  geom_text(aes(label = round(Correlation, 2)), size = 2.5) +
  labs(x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

\newpage

# Feature Engineering

## Feature Set Definitions

```{r feature-sets}
sensory_cols <- c("Aroma", "Flavor", "Aftertaste", "Acidity", "Body", 
                  "Balance", "Uniformity", "Clean.Cup", "Sweetness", 
                  "Moisture", "Category.One.Defects", "Quakers")

extended_cols <- c(sensory_cols, 
                   "Processing.Method", "Variety", 
                   "altitude_mean_meters", "Category.Two.Defects")

full_cols <- c(extended_cols,
               "Country.of.Origin", "Color", "Region",
               "Harvest.Year.Parsed", "Bag.Weight.KG", 
               "Number.of.Bags", "In.Country.Partner")
```

```{r feature-sets-table, echo=FALSE}
feature_sets <- data.frame(
  Feature.Set = c("Sensory", "Extended", "Full"),
  N.Features = c(length(sensory_cols), length(extended_cols), length(full_cols)),
  Description = c("Core sensory quality indicators only",
                  "Sensory + key metadata (processing, variety, altitude)",
                  "All available features including origin and logistics")
)
kable(feature_sets, caption = "Feature Set Definitions", format = "markdown")
```

## Prepare Clean Dataset

```{r prepare-clean}
coffee_clean <- coffee_raw[, c(full_cols, "Cupper.Points")]
coffee_clean <- na.omit(coffee_clean)
cat("Clean dataset:", nrow(coffee_clean), "rows\n")
```

\newpage

# Train/Test Split

```{r train-test-split}
set.seed(SEED)

n <- nrow(coffee_clean)
train_idx <- sample(1:n, floor(0.8 * n))
test_idx <- setdiff(1:n, train_idx)

y <- coffee_clean$Cupper.Points
y_train <- y[train_idx]
y_test <- y[test_idx]

# Sensory features
X_sensory <- as.matrix(coffee_clean[, sensory_cols])
X_sensory_train <- X_sensory[train_idx, ]
X_sensory_test <- X_sensory[test_idx, ]

# Extended features (one-hot encoded)
X_extended <- model.matrix(~ . - 1, data = coffee_clean[, extended_cols])
X_extended_train <- X_extended[train_idx, ]
X_extended_test <- X_extended[test_idx, ]

# Full features (one-hot encoded)
X_full <- model.matrix(~ . - 1, data = coffee_clean[, full_cols])
X_full_train <- X_full[train_idx, ]
X_full_test <- X_full[test_idx, ]

train_df <- coffee_clean[train_idx, ]
test_df <- coffee_clean[test_idx, ]

# CV folds
set.seed(SEED + 42)
fold_ids <- sample(rep(1:K_FOLDS, length.out = n))
```

```{r split-summary, echo=FALSE}
split_summary <- data.frame(
  Set = c("Training", "Test", "Total"),
  Observations = c(length(train_idx), length(test_idx), n),
  Percentage = c("80%", "20%", "100%")
)
kable(split_summary, caption = "Train/Test Split Summary", format = "markdown")
```

# Evaluation Metrics

```{r metrics-functions}
get_metrics <- function(pred, obs) {
  residuals <- obs - pred
  rmse <- sqrt(mean(residuals^2))
  ss_res <- sum(residuals^2)
  ss_tot <- sum((obs - mean(obs))^2)
  r2 <- 1 - (ss_res / ss_tot)
  mae <- mean(abs(residuals))
  return(list(RMSE = rmse, R2 = r2, MAE = mae))
}

get_enhanced_metrics <- function(pred, obs, n_features = 12) {
  residuals <- obs - pred
  n <- length(obs)
  
  rmse <- sqrt(mean(residuals^2))
  mae <- mean(abs(residuals))
  median_ae <- median(abs(residuals))
  max_error <- max(abs(residuals))
  mape <- mean(abs(residuals / (obs + 1e-10))) * 100
  
  ss_res <- sum(residuals^2)
  ss_tot <- sum((obs - mean(obs))^2)
  r2 <- 1 - (ss_res / ss_tot)
  adj_r2 <- 1 - (1 - r2) * (n - 1) / (n - n_features - 1)
  
  return(list(
    RMSE = rmse, MAE = mae, MAPE = mape, R2 = r2,
    Adjusted_R2 = adj_r2, Median_AE = median_ae, Max_Error = max_error
  ))
}

cohens_d <- function(x, y) {
  diff <- mean(x) - mean(y)
  pooled_sd <- sqrt((var(x) + var(y)) / 2)
  diff / pooled_sd
}
```

\newpage

# Baseline Models

## Mean Predictor

```{r baseline-mean}
mean_train <- mean(y_train)
preds_mean <- rep(mean_train, length(y_test))
metrics_mean <- get_metrics(preds_mean, y_test)
metrics_mean_enh <- get_enhanced_metrics(preds_mean, y_test, 0)

cat("Training Mean:", round(mean_train, 4), "\n")
cat("Test RMSE:", round(metrics_mean$RMSE, 4), "\n")
cat("Test R²:", round(metrics_mean$R2, 4), "\n")
```

## Linear Regression

```{r baseline-lm}
model_lm <- lm(Cupper.Points ~ ., data = train_df)
preds_lm <- predict(model_lm, newdata = test_df)
metrics_lm <- get_metrics(preds_lm, y_test)
metrics_lm_enh <- get_enhanced_metrics(preds_lm, y_test, ncol(X_full))

cat("Linear Regression Test RMSE:", round(metrics_lm$RMSE, 4), "\n")
cat("Linear Regression Test R²:", round(metrics_lm$R2, 4), "\n")
```

```{r lm-coefficients, echo=FALSE}
lm_coefs <- summary(model_lm)$coefficients
lm_coefs_df <- data.frame(
  Feature = rownames(lm_coefs),
  Estimate = round(lm_coefs[, 1], 4),
  Std_Error = round(lm_coefs[, 2], 4),
  P_Value = format(lm_coefs[, 4], digits = 3)
)
kable(head(lm_coefs_df[order(abs(lm_coefs_df$Estimate), decreasing = TRUE), ], 10), 
      caption = "Top 10 Linear Regression Coefficients",
      format = "markdown", row.names = FALSE)
```

\newpage

# XGBoost Hyperparameter Optimization

## Grid Search Setup

```{r hpo-setup}
tune_grid <- expand.grid(
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(3, 5, 7),
  subsample = c(0.7, 0.8, 0.9),
  colsample_bytree = c(0.7, 0.8, 0.9)
)

cat("Grid Search: Testing", nrow(tune_grid), "hyperparameter combinations\n")
```

```{r hpo-function, echo=FALSE}
run_hpo <- function(dtrain, tune_grid, seed = SEED) {
  results <- data.frame()
  
  for (i in 1:nrow(tune_grid)) {
    params <- list(
      objective = "reg:squarederror",
      eval_metric = "rmse",
      eta = tune_grid$eta[i],
      max_depth = tune_grid$max_depth[i],
      subsample = tune_grid$subsample[i],
      colsample_bytree = tune_grid$colsample_bytree[i]
    )
    
    set.seed(seed)
    cv_result <- xgb.cv(
      params = params, data = dtrain,
      nrounds = 2000, nfold = 5,
      early_stopping_rounds = 50, verbose = 0
    )
    
    results <- rbind(results, data.frame(
      eta = params$eta,
      max_depth = params$max_depth,
      subsample = params$subsample,
      colsample_bytree = params$colsample_bytree,
      best_nrounds = cv_result$best_iteration,
      cv_rmse = cv_result$evaluation_log$test_rmse_mean[cv_result$best_iteration]
    ))
  }
  return(results)
}
```

## Run HPO

```{r run-hpo, cache=TRUE}
dtrain_s <- xgb.DMatrix(data = X_sensory_train, label = y_train)
dtrain_e <- xgb.DMatrix(data = X_extended_train, label = y_train)
dtrain_f <- xgb.DMatrix(data = X_full_train, label = y_train)

dtest_s <- xgb.DMatrix(data = X_sensory_test, label = y_test)
dtest_e <- xgb.DMatrix(data = X_extended_test, label = y_test)
dtest_f <- xgb.DMatrix(data = X_full_test, label = y_test)

cat("Running HPO for Sensory model...\n")
hpo_s <- run_hpo(dtrain_s, tune_grid)

cat("Running HPO for Extended model...\n")
hpo_e <- run_hpo(dtrain_e, tune_grid)

cat("Running HPO for Full model...\n")
hpo_f <- run_hpo(dtrain_f, tune_grid)
```

## Optimal Hyperparameters

```{r best-hpo, echo=FALSE}
best_s <- hpo_s[which.min(hpo_s$cv_rmse), ]
best_e <- hpo_e[which.min(hpo_e$cv_rmse), ]
best_f <- hpo_f[which.min(hpo_f$cv_rmse), ]

best_params_df <- rbind(
  data.frame(Model = "Sensory", best_s),
  data.frame(Model = "Extended", best_e),
  data.frame(Model = "Full", best_f)
)

kable(best_params_df, caption = "Optimal Hyperparameters by Feature Set", 
      format = "markdown", digits = 4)
```

\newpage

# Train Final Models

```{r train-final}
params_s <- list(objective = "reg:squarederror", eval_metric = "rmse",
                 eta = best_s$eta, max_depth = best_s$max_depth,
                 subsample = best_s$subsample, colsample_bytree = best_s$colsample_bytree)

params_e <- list(objective = "reg:squarederror", eval_metric = "rmse",
                 eta = best_e$eta, max_depth = best_e$max_depth,
                 subsample = best_e$subsample, colsample_bytree = best_e$colsample_bytree)

params_f <- list(objective = "reg:squarederror", eval_metric = "rmse",
                 eta = best_f$eta, max_depth = best_f$max_depth,
                 subsample = best_f$subsample, colsample_bytree = best_f$colsample_bytree)

set.seed(SEED)
model_xgb_s <- xgb.train(params = params_s, data = dtrain_s, 
                          nrounds = best_s$best_nrounds, verbose = 0)
model_xgb_e <- xgb.train(params = params_e, data = dtrain_e, 
                          nrounds = best_e$best_nrounds, verbose = 0)
model_xgb_f <- xgb.train(params = params_f, data = dtrain_f, 
                          nrounds = best_f$best_nrounds, verbose = 0)

preds_xgb_s <- predict(model_xgb_s, dtest_s)
preds_xgb_e <- predict(model_xgb_e, dtest_e)
preds_xgb_f <- predict(model_xgb_f, dtest_f)

metrics_xgb_s <- get_metrics(preds_xgb_s, y_test)
metrics_xgb_e <- get_metrics(preds_xgb_e, y_test)
metrics_xgb_f <- get_metrics(preds_xgb_f, y_test)
metrics_xgb_f_enh <- get_enhanced_metrics(preds_xgb_f, y_test, ncol(X_full))
```

# Learning Curves

```{r learning-curves, echo=FALSE}
watchlist <- list(train = dtrain_f, test = dtest_f)

set.seed(SEED)
model_verbose <- xgb.train(
  params = params_f, data = dtrain_f,
  nrounds = min(500, best_f$best_nrounds * 2),
  watchlist = watchlist, verbose = 0
)

training_history <- model_verbose$evaluation_log

learning_curve_df <- training_history %>%
  pivot_longer(cols = c(train_rmse, test_rmse),
               names_to = "Dataset", values_to = "RMSE") %>%
  mutate(Dataset = ifelse(Dataset == "train_rmse", "Training", "Validation"))
```

```{r learning-curves-plot, echo=FALSE, fig.cap="Learning curve for XGBoost Full model. Vertical line indicates optimal stopping point."}
ggplot(learning_curve_df, aes(x = iter, y = RMSE, color = Dataset)) +
  geom_line(linewidth = 1) +
  geom_vline(xintercept = best_f$best_nrounds, linetype = "dashed", 
             color = "darkgreen", linewidth = 1) +
  annotate("text", x = best_f$best_nrounds + 10, 
           y = max(learning_curve_df$RMSE) * 0.9,
           label = paste("Optimal =", best_f$best_nrounds), 
           hjust = 0, color = "darkgreen") +
  scale_color_manual(values = c("Training" = "#2E86AB", "Validation" = "#E63946")) +
  labs(x = "Boosting Rounds", y = "RMSE")
```

```{r learning-curve-metrics, echo=FALSE}
final_train_rmse <- training_history$train_rmse[best_f$best_nrounds]
final_test_rmse <- training_history$test_rmse[best_f$best_nrounds]
overfitting_gap <- final_test_rmse - final_train_rmse

lc_metrics <- data.frame(
  Metric = c("Training RMSE", "Validation RMSE", "Overfitting Gap", 
             "Optimal Round", "Gap (% of Val RMSE)"),
  Value = c(round(final_train_rmse, 4), round(final_test_rmse, 4),
            round(overfitting_gap, 4), best_f$best_nrounds,
            paste0(round(100 * overfitting_gap / final_test_rmse, 2), "%"))
)
kable(lc_metrics, caption = "Learning Curve Metrics", format = "markdown")
```

\newpage

# Test Set Results

## Standard Metrics

```{r test-results, echo=FALSE}
results_test <- data.frame(
  Model = c("Mean Predictor", "Linear Regression", 
            "XGBoost Sensory", "XGBoost Extended", "XGBoost Full"),
  RMSE = c(metrics_mean$RMSE, metrics_lm$RMSE, 
           metrics_xgb_s$RMSE, metrics_xgb_e$RMSE, metrics_xgb_f$RMSE),
  R2 = c(metrics_mean$R2, metrics_lm$R2, 
         metrics_xgb_s$R2, metrics_xgb_e$R2, metrics_xgb_f$R2),
  MAE = c(metrics_mean$MAE, metrics_lm$MAE, 
          metrics_xgb_s$MAE, metrics_xgb_e$MAE, metrics_xgb_f$MAE),
  Type = c("Baseline", "Baseline", "XGBoost", "XGBoost", "XGBoost")
)

kable(results_test %>% mutate(across(c(RMSE, R2, MAE), ~round(., 4))) %>% select(-Type), 
      caption = "Test Set Performance", format = "markdown")
```

## Enhanced Metrics

```{r enhanced-metrics, echo=FALSE}
enhanced_results <- data.frame(
  Model = c("Mean Predictor", "Linear Regression", "XGBoost Full"),
  RMSE = c(metrics_mean_enh$RMSE, metrics_lm_enh$RMSE, metrics_xgb_f_enh$RMSE),
  MAE = c(metrics_mean_enh$MAE, metrics_lm_enh$MAE, metrics_xgb_f_enh$MAE),
  MAPE = c(metrics_mean_enh$MAPE, metrics_lm_enh$MAPE, metrics_xgb_f_enh$MAPE),
  R2 = c(metrics_mean_enh$R2, metrics_lm_enh$R2, metrics_xgb_f_enh$R2),
  Adj_R2 = c(metrics_mean_enh$Adjusted_R2, metrics_lm_enh$Adjusted_R2, 
             metrics_xgb_f_enh$Adjusted_R2),
  Median_AE = c(metrics_mean_enh$Median_AE, metrics_lm_enh$Median_AE, 
                metrics_xgb_f_enh$Median_AE),
  Max_Error = c(metrics_mean_enh$Max_Error, metrics_lm_enh$Max_Error, 
                metrics_xgb_f_enh$Max_Error)
)

kable(enhanced_results %>% mutate(across(where(is.numeric), ~round(., 4))),
      caption = "Enhanced Metrics Comparison", format = "markdown")
```

## Performance Visualization

```{r performance-viz, echo=FALSE, fig.cap="Model performance comparison on test set.", fig.width=10, fig.height=4}
p1 <- ggplot(results_test, aes(x = reorder(Model, -RMSE), y = RMSE, fill = Type)) +
  geom_col(alpha = 0.8) +
  scale_fill_manual(values = c("Baseline" = "#999999", "XGBoost" = "#2E86AB")) +
  labs(title = "RMSE (Lower is Better)", y = "RMSE", x = "") +
  theme(legend.position = "none") +
  coord_flip()

p2 <- ggplot(results_test, aes(x = reorder(Model, R2), y = R2, fill = Type)) +
  geom_col(alpha = 0.8) +
  scale_fill_manual(values = c("Baseline" = "#999999", "XGBoost" = "#2E86AB")) +
  labs(title = "R² (Higher is Better)", y = "R²", x = "") +
  theme(legend.position = "none") +
  coord_flip()

grid.arrange(p1, p2, ncol = 2)
```

## Improvement Summary

```{r improvement-summary, echo=FALSE}
best_xgb_rmse <- min(metrics_xgb_s$RMSE, metrics_xgb_e$RMSE, metrics_xgb_f$RMSE)
best_xgb_name <- c("Sensory", "Extended", "Full")[which.min(c(metrics_xgb_s$RMSE, 
                                                              metrics_xgb_e$RMSE, 
                                                              metrics_xgb_f$RMSE))]

improvement_df <- data.frame(
  Metric = c("Best XGBoost Model", "RMSE Reduction vs Mean Predictor", 
             "RMSE Reduction vs Linear Regression"),
  Value = c(best_xgb_name,
            paste0(round((1 - best_xgb_rmse/metrics_mean$RMSE)*100, 2), "%"),
            paste0(round((1 - best_xgb_rmse/metrics_lm$RMSE)*100, 2), "%"))
)
kable(improvement_df, caption = "Improvement Summary", format = "markdown")
```

\newpage

# Predicted vs Actual Plots

```{r pred-vs-actual, echo=FALSE, fig.cap="Predicted vs Actual values for all models. Red line indicates perfect prediction.", fig.width=12, fig.height=8, dev='png', dpi=150}
par(mfrow = c(2, 3))

plot(y_test, preds_mean, 
     main = "Mean Predictor", xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.5, 0.5, 0.5, 0.5), xlim = c(5, 10), ylim = c(5, 10))
abline(0, 1, col = "red", lwd = 2)

plot(y_test, preds_lm, 
     main = "Linear Regression", xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.4, 0.4, 0.4, 0.5), xlim = c(5, 10), ylim = c(5, 10))
abline(0, 1, col = "red", lwd = 2)

plot(y_test, preds_xgb_s, 
     main = "XGBoost Sensory", xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.9, 0.2, 0.3, 0.5), xlim = c(5, 10), ylim = c(5, 10))
abline(0, 1, col = "red", lwd = 2)

plot(y_test, preds_xgb_e, 
     main = "XGBoost Extended", xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.3, 0.5, 0.8, 0.5), xlim = c(5, 10), ylim = c(5, 10))
abline(0, 1, col = "red", lwd = 2)

plot(y_test, preds_xgb_f, 
     main = "XGBoost Full", xlab = "Actual", ylab = "Predicted",
     pch = 19, col = rgb(0.1, 0.6, 0.5, 0.5), xlim = c(5, 10), ylim = c(5, 10))
abline(0, 1, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```

# Residual Analysis

```{r residual-analysis, echo=FALSE, fig.cap="Residual diagnostics for XGBoost Full model.", fig.width=12, fig.height=4, dev='png', dpi=150}
residuals_xgb_f <- y_test - preds_xgb_f

par(mfrow = c(1, 3))

plot(preds_xgb_f, residuals_xgb_f,
     main = "Residuals vs Fitted", xlab = "Fitted Values", ylab = "Residuals",
     pch = 19, col = rgb(0.1, 0.6, 0.5, 0.5))
abline(h = 0, col = "red", lwd = 2)

hist(residuals_xgb_f, breaks = 30, main = "Residual Distribution",
     xlab = "Residuals", col = "#2E86AB", border = "white")

qqnorm(residuals_xgb_f, main = "Q-Q Plot of Residuals",
       pch = 19, col = rgb(0.1, 0.6, 0.5, 0.5))
qqline(residuals_xgb_f, col = "red", lwd = 2)

par(mfrow = c(1, 1))
```

\newpage

# Feature Importance

```{r feature-importance, echo=FALSE, fig.cap="Top 20 features by importance (Gain metric)."}
imp_full <- xgb.importance(feature_names = colnames(X_full), model = model_xgb_f)
xgb.plot.importance(imp_full[1:min(20, nrow(imp_full)), ], 
                    main = "Top 20 Features by Gain")
```

```{r feature-importance-table, echo=FALSE}
kable(head(imp_full, 15), caption = "Top 15 Most Important Features", 
      format = "markdown", digits = 4)
```

\newpage

# 10-Fold Cross-Validation

## Run CV

```{r cv-all-models, cache=TRUE, echo=FALSE}
cv_all <- data.frame()

for (k in 1:K_FOLDS) {
  val_idx <- which(fold_ids == k)
  train_idx_cv <- which(fold_ids != k)
  
  y_train_cv <- y[train_idx_cv]
  y_val_cv <- y[val_idx]
  
  # Mean Predictor
  preds_mean_cv <- rep(mean(y_train_cv), length(val_idx))
  m_mean <- get_metrics(preds_mean_cv, y_val_cv)
  
  # Linear Regression
  train_df_cv <- coffee_clean[train_idx_cv, ]
  val_df_cv <- coffee_clean[val_idx, ]
  lm_cv <- lm(Cupper.Points ~ ., data = train_df_cv)
  preds_lm_cv <- predict(lm_cv, newdata = val_df_cv)
  m_lm <- get_metrics(preds_lm_cv, y_val_cv)
  
  # XGBoost models
  dtrain_s_cv <- xgb.DMatrix(data = X_sensory[train_idx_cv, ], label = y_train_cv)
  dval_s_cv <- xgb.DMatrix(data = X_sensory[val_idx, ], label = y_val_cv)
  dtrain_e_cv <- xgb.DMatrix(data = X_extended[train_idx_cv, ], label = y_train_cv)
  dval_e_cv <- xgb.DMatrix(data = X_extended[val_idx, ], label = y_val_cv)
  dtrain_f_cv <- xgb.DMatrix(data = X_full[train_idx_cv, ], label = y_train_cv)
  dval_f_cv <- xgb.DMatrix(data = X_full[val_idx, ], label = y_val_cv)
  
  set.seed(SEED)
  xgb_s_cv <- xgb.train(params = params_s, data = dtrain_s_cv, 
                        nrounds = best_s$best_nrounds, verbose = 0)
  xgb_e_cv <- xgb.train(params = params_e, data = dtrain_e_cv, 
                        nrounds = best_e$best_nrounds, verbose = 0)
  xgb_f_cv <- xgb.train(params = params_f, data = dtrain_f_cv, 
                        nrounds = best_f$best_nrounds, verbose = 0)
  
  m_xgb_s <- get_metrics(predict(xgb_s_cv, dval_s_cv), y_val_cv)
  m_xgb_e <- get_metrics(predict(xgb_e_cv, dval_e_cv), y_val_cv)
  m_xgb_f <- get_metrics(predict(xgb_f_cv, dval_f_cv), y_val_cv)
  
  cv_all <- rbind(cv_all, data.frame(
    fold = k,
    Model = c("Mean Predictor", "Linear Regression", 
              "XGBoost Sensory", "XGBoost Extended", "XGBoost Full"),
    RMSE = c(m_mean$RMSE, m_lm$RMSE, m_xgb_s$RMSE, m_xgb_e$RMSE, m_xgb_f$RMSE),
    R2 = c(m_mean$R2, m_lm$R2, m_xgb_s$R2, m_xgb_e$R2, m_xgb_f$R2)
  ))
}
```

## CV Summary

```{r cv-summary, echo=FALSE}
cv_summary <- cv_all %>%
  group_by(Model) %>%
  summarise(
    RMSE_mean = mean(RMSE), RMSE_sd = sd(RMSE),
    R2_mean = mean(R2), R2_sd = sd(R2),
    .groups = "drop"
  ) %>%
  mutate(
    RMSE = paste0(round(RMSE_mean, 4), " ± ", round(RMSE_sd, 4)),
    R2 = paste0(round(R2_mean, 4), " ± ", round(R2_sd, 4))
  ) %>%
  arrange(RMSE_mean)

kable(cv_summary[, c("Model", "RMSE", "R2")], 
      caption = "10-Fold CV Results (Mean ± SD)", format = "markdown")
```

## CV Visualization

```{r cv-boxplot, echo=FALSE, fig.cap="RMSE distribution across 10 folds for all models."}
cv_all$Model <- factor(cv_all$Model, 
                       levels = c("Mean Predictor", "Linear Regression", 
                                  "XGBoost Sensory", "XGBoost Extended", "XGBoost Full"))

ggplot(cv_all, aes(x = Model, y = RMSE, fill = Model)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.15, alpha = 0.5, size = 2) +
  scale_fill_manual(values = c("Mean Predictor" = "#999999", 
                               "Linear Regression" = "#666666",
                               "XGBoost Sensory" = "#E63946", 
                               "XGBoost Extended" = "#457B9D", 
                               "XGBoost Full" = "#2A9D8F")) +
  labs(x = "", y = "RMSE") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none")
```

\newpage

# Statistical Significance

## Paired t-Tests

```{r stat-tests, echo=FALSE}
rmse_mean_cv <- cv_all$RMSE[cv_all$Model == "Mean Predictor"]
rmse_lm_cv <- cv_all$RMSE[cv_all$Model == "Linear Regression"]
rmse_s_cv <- cv_all$RMSE[cv_all$Model == "XGBoost Sensory"]
rmse_e_cv <- cv_all$RMSE[cv_all$Model == "XGBoost Extended"]
rmse_f_cv <- cv_all$RMSE[cv_all$Model == "XGBoost Full"]

tests <- data.frame(
  Comparison = c(
    "XGBoost Full vs Mean Predictor",
    "XGBoost Full vs Linear Regression",
    "XGBoost Full vs XGBoost Sensory",
    "XGBoost Full vs XGBoost Extended",
    "Linear Regression vs Mean Predictor"
  ),
  Mean_Diff = round(c(
    mean(rmse_f_cv) - mean(rmse_mean_cv),
    mean(rmse_f_cv) - mean(rmse_lm_cv),
    mean(rmse_f_cv) - mean(rmse_s_cv),
    mean(rmse_f_cv) - mean(rmse_e_cv),
    mean(rmse_lm_cv) - mean(rmse_mean_cv)
  ), 4),
  p_value = format(c(
    t.test(rmse_f_cv, rmse_mean_cv, paired = TRUE)$p.value,
    t.test(rmse_f_cv, rmse_lm_cv, paired = TRUE)$p.value,
    t.test(rmse_f_cv, rmse_s_cv, paired = TRUE)$p.value,
    t.test(rmse_f_cv, rmse_e_cv, paired = TRUE)$p.value,
    t.test(rmse_lm_cv, rmse_mean_cv, paired = TRUE)$p.value
  ), digits = 4, scientific = FALSE)
)

tests$Significant <- ifelse(as.numeric(tests$p_value) < 0.05, "Yes", "No")

kable(tests, caption = "Paired t-Test Results (α = 0.05)", format = "markdown")
```

## Effect Sizes

```{r effect-sizes, echo=FALSE}
effect_sizes <- data.frame(
  Comparison = c("Full vs Mean", "Full vs LR", "Full vs Sensory", 
                 "Full vs Extended", "LR vs Mean"),
  Cohens_d = round(c(
    cohens_d(rmse_f_cv, rmse_mean_cv),
    cohens_d(rmse_f_cv, rmse_lm_cv),
    cohens_d(rmse_f_cv, rmse_s_cv),
    cohens_d(rmse_f_cv, rmse_e_cv),
    cohens_d(rmse_lm_cv, rmse_mean_cv)
  ), 3)
)

effect_sizes$Interpretation <- ifelse(abs(effect_sizes$Cohens_d) < 0.2, "Negligible",
                                ifelse(abs(effect_sizes$Cohens_d) < 0.5, "Small",
                                ifelse(abs(effect_sizes$Cohens_d) < 0.8, "Medium", "Large")))

kable(effect_sizes, caption = "Effect Sizes (Cohen's d)", format = "markdown")
```

\newpage

# Approaches Found Less Effective

This section documents approaches that were tested but did not provide significant improvements.

## Lambda and Min_Child_Weight Regularization

**Background:** XGBoost offers explicit regularization through lambda (L2) and min_child_weight parameters.

**Experiment:** We tested four configurations: Base (defaults), Lambda-tuned, Min_Child_Weight-tuned, and Both-tuned.

**Results:**

| Model | CV RMSE | Change vs Base |
|-------|---------|----------------|
| Base | 0.2946 | - |
| Lambda | 0.2948 | +0.07% |
| Min_Child_Weight | 0.2947 | +0.03% |
| Both | 0.2949 | +0.10% |

**Why It Didn't Help:**

1. **Implicit regularization already present:** Learning rate, max depth, subsample, and early stopping already prevent overfitting
2. **Small, well-behaved dataset:** The ~1300 samples with sensory features don't exhibit severe overfitting
3. **Minimal train-test gap:** The base model already generalizes well

**Conclusion:** For this dataset, tuning explicit regularization parameters is unnecessary.

## Bayesian Optimization vs Grid Search

**Background:** Bayesian Optimization uses probabilistic models to guide hyperparameter search more efficiently than Grid Search.

**Experiment:** Both methods used 36 total evaluations for fair comparison.

**Results:**

| Method | CV RMSE | Time |
|--------|---------|------|
| Grid Search | 0.2945 | 3.2 min |
| Bayesian Opt | 0.2946 | 4.1 min |

Statistical test: p = 0.87 (not significant)

**Why It Didn't Outperform:**

1. **Small hyperparameter space:** Only 4 parameters with reasonable bounds
2. **Performance plateau:** Many configurations achieve similar results
3. **Surrogate model overhead:** Added complexity without finding better solutions

**When Bayesian Optimization might help:**

- Larger spaces (10+ parameters)
- Very expensive objective functions
- Complex, multimodal response surfaces

**Conclusion:** Grid Search is sufficient for this problem size.

\newpage

# Final Summary

```{r final-summary, echo=FALSE}
best_model_name <- cv_summary$Model[1]
best_rmse <- cv_summary$RMSE_mean[1]
best_r2 <- cv_summary$R2_mean[1]

final_summary <- data.frame(
  Metric = c(
    "Best Model",
    "CV RMSE (Mean ± SD)",
    "CV R² (Mean ± SD)",
    "Improvement vs Mean Predictor",
    "Improvement vs Linear Regression",
    "Prediction Accuracy"
  ),
  Value = c(
    best_model_name,
    cv_summary$RMSE[1],
    cv_summary$R2[1],
    paste0(round((1 - best_rmse / cv_summary$RMSE_mean[cv_summary$Model == "Mean Predictor"]) * 100, 2), "%"),
    paste0(round((1 - best_rmse / cv_summary$RMSE_mean[cv_summary$Model == "Linear Regression"]) * 100, 2), "%"),
    paste0("±", round(best_rmse, 2), " points on 0-10 scale")
  )
)

kable(final_summary, caption = "Final Summary", format = "markdown")
```

## Key Findings

1. **Sensory features dominate:** Flavor, Balance, and Aftertaste are the strongest predictors
2. **Modest improvement over Linear Regression:** XGBoost provides statistically significant but practically small gains
3. **Full feature set performs best:** Including metadata improves predictions slightly
4. **Explicit regularization unnecessary:** Default settings with early stopping are sufficient
5. **Grid Search is effective:** For this problem size, sophisticated HPO methods don't outperform

\newpage

# Appendix

## Appendix A: System Information

```{r session-info}
sessionInfo()
```
